import requests

def test_crawler():
    # ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ•ãƒ¬ã‚¢ãªã©ã®é®æ–­ã®å½±éŸ¿ã‚’å—ã‘ãªã„ Hacker News ã®å…¬å¼å…¬é–‹APIã‚’ã¤ã‹ã£ã¦
    # ã€Œã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‹ã‚‰æœ€æ–°ã®æ±‚äººæƒ…å ±ã‚’JSONã§å–å¾—ã™ã‚‹ã€ã‚³ã‚¢éƒ¨åˆ†ã®å®Ÿè¨¼ã‚’è¡Œã„ã¾ã™
    url_stories = "https://hacker-news.firebaseio.com/v0/jobstories.json"
    print(f"ğŸ” æƒ…å ±å–å¾—å…ˆURL: {url_stories}")
    print("--------------------------------------------------")

    try:
        # æœ€æ–°ã®æ±‚äººIDãƒªã‚¹ãƒˆã‚’å–å¾—
        res_stories = requests.get(url_stories, timeout=10)
        
        if res_stories.status_code == 200:
            job_ids = res_stories.json()
            
            if not job_ids:
                print("âš ï¸ æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return
                
            print(f"âœ… æ±‚äººIDãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã—ãŸã€‚ä¸Šä½3ä»¶ã®è©³ç´°ã‚’ãƒ•ã‚§ãƒƒãƒã—ã¾ã™ï¼\n")
            
            for i, job_id in enumerate(job_ids[:3], 1):
                detail_url = f"https://hacker-news.firebaseio.com/v0/item/{job_id}.json"
                res_detail = requests.get(detail_url, timeout=10).json()
                
                title = res_detail.get('title', 'No title')
                link = res_detail.get('url', f"https://news.ycombinator.com/item?id={job_id}")
                
                print(f"[{i}] {title}")
                print(f"    ãƒªãƒ³ã‚¯: {link}\n")
        else:
            print(f"âš ï¸ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {res_stories.status_code}")
            
    except Exception as e:
        print(f"âŒ Webæƒ…å ±ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    print("ğŸš€ Webæƒ…å ±å–å¾—ãƒ†ã‚¹ãƒˆï¼ˆæƒ…å ±åé›†ã‚³ã‚¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")
    test_crawler()
    print("âœ… ãƒ†ã‚¹ãƒˆå®Œäº†")
